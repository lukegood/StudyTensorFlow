{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、边看边写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-e38aab5b331a>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-e38aab5b331a>\"\u001b[1;36m, line \u001b[1;32m47\u001b[0m\n\u001b[1;33m    w1.assign_sub(lr * grads[0])  # 参数w1自更新\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target\n",
    "\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_data)\n",
    "np.random.seed(116)\n",
    "\n",
    "x_train = x_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_train = y_data[:-30]\n",
    "y_test = y_data[-30:]\n",
    "\n",
    "x_train = tf.cast(x_train, dtype=tf.float32)\n",
    "x_test = tf.cast(x_test, dtype=tf.float32)\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4,3], stddev=0.1, seed=1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))\n",
    "\n",
    "lr = 0.1\n",
    "train_loss_result = []\n",
    "test_acc = []\n",
    "epoch = 500\n",
    "loss_all = 0\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    for step, (x_train, y_train) in enumerate(train_db):\n",
    "#         with tf.GradientTape as tape:\n",
    "#             y = tf.matmul(x_train, w1) + b1\n",
    "#             y = tf.nn.softmax(y)\n",
    "#             y_ = tf.one_hot(y_train, depth = 3)\n",
    "#             loss = tf.reduce_mean(tf.square(y - y_))\n",
    "#             loss_all += loss.numpy()\n",
    "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
    "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
    "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
    "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
    "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
    "            loss_all += loss.numpy(\n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "        \n",
    "        #w1.assign_sub(lr * grads[0])\n",
    "        w1.assign_sub(lr * grads[0])  # 参数w1自更新\n",
    "        b1.assign_sub(lr * grads[1])\n",
    "        \n",
    "    print(\"Epoch{}, loss{}\").format(epoch, loss_all/4)\n",
    "    train_loss_result.append(loss_all / 4)\n",
    "    loss_all = 0\n",
    "    \n",
    "    total_correct, total_number = 0, 0\n",
    "    for x_test, y_test in test_db:\n",
    "        y = tf.matmul(x_test, w1) + b1\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y, axis = 1)\n",
    "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
    "        correct = tf.cast(tf.equal(pred, y_test),dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        total_correct += int(correct)\n",
    "        total_number += x_test.shape[0]\n",
    "        \n",
    "    acc = total_correct / total_number\n",
    "    test_acc.append(acc)\n",
    "    print(\"Test_acc:\", acc)\n",
    "    print(\"--------------------------\")\n",
    "    \n",
    "# 绘制 loss 曲线\n",
    "plt.title('Loss Function Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Loss')  # y轴变量名称\n",
    "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
    "plt.legend()  # 画出曲线图标\n",
    "plt.show()  # 画出图像\n",
    "\n",
    "# 绘制 Accuracy 曲线\n",
    "plt.title('Acc Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Acc')  # y轴变量名称\n",
    "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.2821310982108116\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 1, loss: 0.25459613651037216\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 2, loss: 0.22570249810814857\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 3, loss: 0.21028399839997292\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 4, loss: 0.19942265003919601\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 5, loss: 0.18873638659715652\n",
      "Test_acc: 0.5\n",
      "--------------------------\n",
      "Epoch 6, loss: 0.17851299047470093\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 7, loss: 0.16922876238822937\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 8, loss: 0.16107673570513725\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 9, loss: 0.15404685214161873\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 10, loss: 0.14802726358175278\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 11, loss: 0.14287303388118744\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 12, loss: 0.1384414155036211\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 13, loss: 0.13460607640445232\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 14, loss: 0.1312607228755951\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 15, loss: 0.12831821851432323\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 16, loss: 0.12570794485509396\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 17, loss: 0.12337298691272736\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 18, loss: 0.12126746028661728\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 19, loss: 0.11935432627797127\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 20, loss: 0.11760355532169342\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 21, loss: 0.11599067598581314\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 22, loss: 0.11449568346142769\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 23, loss: 0.11310207471251488\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 24, loss: 0.11179621517658234\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 25, loss: 0.11056671850383282\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 26, loss: 0.10940408147871494\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 27, loss: 0.10830027982592583\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 28, loss: 0.10724854655563831\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 29, loss: 0.10624312795698643\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 30, loss: 0.1052790954709053\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 31, loss: 0.10435221716761589\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 32, loss: 0.10345886461436749\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 33, loss: 0.1025958750396967\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 34, loss: 0.10176052711904049\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 35, loss: 0.10095042549073696\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 36, loss: 0.10016347281634808\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 37, loss: 0.09939784556627274\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 38, loss: 0.09865193255245686\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 39, loss: 0.09792428836226463\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 40, loss: 0.09721365384757519\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 41, loss: 0.09651889279484749\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 42, loss: 0.09583901427686214\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 43, loss: 0.09517310187220573\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 44, loss: 0.09452036768198013\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 45, loss: 0.09388007037341595\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 46, loss: 0.09325155802071095\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 47, loss: 0.09263424947857857\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 48, loss: 0.09202759526669979\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 49, loss: 0.09143111668527126\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 50, loss: 0.09084436483681202\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 51, loss: 0.09026693738996983\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 52, loss: 0.08969846367835999\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 53, loss: 0.08913860470056534\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 54, loss: 0.08858705125749111\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 55, loss: 0.08804351650178432\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 56, loss: 0.08750772848725319\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 57, loss: 0.08697944320738316\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 58, loss: 0.08645843155682087\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 59, loss: 0.08594449050724506\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 60, loss: 0.08543741330504417\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 61, loss: 0.08493702299892902\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 62, loss: 0.08444313332438469\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 63, loss: 0.08395559899508953\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 64, loss: 0.08347426168620586\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 65, loss: 0.0829989816993475\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 66, loss: 0.08252961561083794\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 67, loss: 0.08206603676080704\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 68, loss: 0.08160812966525555\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 69, loss: 0.08115578629076481\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 70, loss: 0.08070887997746468\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 71, loss: 0.08026731573045254\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 72, loss: 0.07983099482953548\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 73, loss: 0.0793998222798109\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 74, loss: 0.07897370308637619\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 75, loss: 0.07855254970490932\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 76, loss: 0.0781362783163786\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 77, loss: 0.07772481255233288\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 78, loss: 0.07731807045638561\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 79, loss: 0.07691597566008568\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 80, loss: 0.07651845552027225\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 81, loss: 0.07612544298171997\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 82, loss: 0.07573686353862286\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 83, loss: 0.07535265572369099\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 84, loss: 0.07497275806963444\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 85, loss: 0.0745970867574215\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 86, loss: 0.07422560080885887\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 87, loss: 0.07385823223739862\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 88, loss: 0.07349492330104113\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 89, loss: 0.07313562370836735\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 90, loss: 0.07278027012944221\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 91, loss: 0.0724288010969758\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 92, loss: 0.07208117935806513\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, loss: 0.07173734065145254\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 94, loss: 0.07139724306762218\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 95, loss: 0.07106082048267126\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 96, loss: 0.07072803936898708\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 97, loss: 0.0703988391906023\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 98, loss: 0.07007318455725908\n",
      "Test_acc: 0.8333333333333334\n",
      "--------------------------\n",
      "Epoch 99, loss: 0.06975101213902235\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 100, loss: 0.06943229306489229\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 101, loss: 0.06911697518080473\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 102, loss: 0.06880501005798578\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 103, loss: 0.06849635299295187\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 104, loss: 0.06819096300750971\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 105, loss: 0.06788880284875631\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 106, loss: 0.0675898278132081\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 107, loss: 0.06729398854076862\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 108, loss: 0.06700124405324459\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 109, loss: 0.06671156454831362\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 110, loss: 0.06642491649836302\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 111, loss: 0.06614124123007059\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 112, loss: 0.0658605145290494\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 113, loss: 0.06558268796652555\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 114, loss: 0.06530773546546698\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 115, loss: 0.06503560673445463\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 116, loss: 0.06476627755910158\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 117, loss: 0.06449970323592424\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 118, loss: 0.06423585768789053\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 119, loss: 0.0639747017994523\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 120, loss: 0.06371620204299688\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 121, loss: 0.0634603202342987\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 122, loss: 0.06320702657103539\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 123, loss: 0.06295627821236849\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 124, loss: 0.06270805187523365\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 125, loss: 0.06246231682598591\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 126, loss: 0.06221904419362545\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 127, loss: 0.06197819113731384\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 128, loss: 0.061739737167954445\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 129, loss: 0.06150364875793457\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 130, loss: 0.06126989144831896\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 131, loss: 0.061038440093398094\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 132, loss: 0.06080926675349474\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 133, loss: 0.06058233417570591\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 134, loss: 0.06035762373358011\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 135, loss: 0.060135108418762684\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 136, loss: 0.05991475656628609\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 137, loss: 0.05969653092324734\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 138, loss: 0.059480419382452965\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 139, loss: 0.05926638934761286\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 140, loss: 0.0590544156730175\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 141, loss: 0.05884446669369936\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 142, loss: 0.05863652750849724\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 143, loss: 0.058430569246411324\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 144, loss: 0.058226561173796654\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 145, loss: 0.05802448280155659\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 146, loss: 0.05782431084662676\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 147, loss: 0.0576260294765234\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 148, loss: 0.05742959212511778\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 149, loss: 0.05723499320447445\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 150, loss: 0.05704221408814192\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 151, loss: 0.05685122311115265\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 152, loss: 0.056661998853087425\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 153, loss: 0.05647451803088188\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 154, loss: 0.05628876481205225\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 155, loss: 0.056104715913534164\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 156, loss: 0.05592234618961811\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 157, loss: 0.05574163142591715\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 158, loss: 0.055562565103173256\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 159, loss: 0.055385115556418896\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 160, loss: 0.05520927254110575\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 161, loss: 0.055035010911524296\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 162, loss: 0.05486230552196503\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 163, loss: 0.05469114612787962\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 164, loss: 0.054521508514881134\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 165, loss: 0.0543533768504858\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 166, loss: 0.05418673437088728\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 167, loss: 0.05402155313640833\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 168, loss: 0.0538578387349844\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 169, loss: 0.05369554739445448\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 170, loss: 0.05353467911481857\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 171, loss: 0.05337520595639944\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 172, loss: 0.053217110224068165\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 173, loss: 0.05306038912385702\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 174, loss: 0.05290501657873392\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 175, loss: 0.05275097955018282\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 176, loss: 0.052598259411752224\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 177, loss: 0.05244683753699064\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 178, loss: 0.052296705543994904\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 179, loss: 0.052147846668958664\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 180, loss: 0.052000245079398155\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 181, loss: 0.05185388308018446\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 182, loss: 0.05170875322073698\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 183, loss: 0.051564838737249374\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 184, loss: 0.05142211355268955\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 185, loss: 0.05128058139234781\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 186, loss: 0.05114021431654692\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 187, loss: 0.05100100859999657\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 188, loss: 0.05086294189095497\n",
      "Test_acc: 1.0\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189, loss: 0.05072601139545441\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 190, loss: 0.050590197555720806\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 191, loss: 0.05045549105852842\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 192, loss: 0.05032187048345804\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 193, loss: 0.05018933676183224\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 194, loss: 0.05005786754190922\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 195, loss: 0.049927457235753536\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 196, loss: 0.049798084422945976\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 197, loss: 0.04966974724084139\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 198, loss: 0.049542427994310856\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 199, loss: 0.04941612295806408\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 200, loss: 0.049290806986391544\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 201, loss: 0.0491664744913578\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 202, loss: 0.04904312081634998\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 203, loss: 0.048920733854174614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 204, loss: 0.04879929218441248\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 205, loss: 0.04867880418896675\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 206, loss: 0.048559242859482765\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 207, loss: 0.048440602608025074\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 208, loss: 0.04832287039607763\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 209, loss: 0.04820604622364044\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 210, loss: 0.04809011146426201\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 211, loss: 0.04797505401074886\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 212, loss: 0.04786087851971388\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 213, loss: 0.04774755798280239\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 214, loss: 0.04763508774340153\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 215, loss: 0.04752346966415644\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 216, loss: 0.0474126860499382\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 217, loss: 0.047302717342972755\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 218, loss: 0.047193583101034164\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 219, loss: 0.04708524141460657\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 220, loss: 0.04697770904749632\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 221, loss: 0.046870965510606766\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 222, loss: 0.04676500242203474\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 223, loss: 0.046659817919135094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 224, loss: 0.04655539244413376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 225, loss: 0.046451729722321033\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 226, loss: 0.046348826959729195\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 227, loss: 0.04624664969742298\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 228, loss: 0.046145216561853886\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 229, loss: 0.046044507063925266\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 230, loss: 0.04594451654702425\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 231, loss: 0.045845236629247665\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 232, loss: 0.045746663585305214\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 233, loss: 0.0456487825140357\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 234, loss: 0.04555159993469715\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 235, loss: 0.04545509163290262\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 236, loss: 0.045359267853200436\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 237, loss: 0.045264105312526226\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 238, loss: 0.04516960773617029\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 239, loss: 0.04507576674222946\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 240, loss: 0.044982568360865116\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 241, loss: 0.044890021905303\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 242, loss: 0.044798110611736774\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 243, loss: 0.04470680933445692\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 244, loss: 0.04461614973843098\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 245, loss: 0.04452610481530428\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 246, loss: 0.04443667642772198\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 247, loss: 0.044347845017910004\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 248, loss: 0.044259609654545784\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 249, loss: 0.044171969406306744\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 250, loss: 0.04408491123467684\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 251, loss: 0.04399843979626894\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 252, loss: 0.043912542052567005\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 253, loss: 0.04382720962166786\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 254, loss: 0.043742443434894085\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 255, loss: 0.043658243492245674\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 256, loss: 0.04357459023594856\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 257, loss: 0.043491486459970474\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 258, loss: 0.04340892378240824\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 259, loss: 0.04332689754664898\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 260, loss: 0.043245408684015274\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 261, loss: 0.04316443484276533\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 262, loss: 0.043083999305963516\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 263, loss: 0.043004064820706844\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 264, loss: 0.04292465094476938\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 265, loss: 0.04284574370831251\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 266, loss: 0.04276733845472336\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 267, loss: 0.04268943518400192\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 268, loss: 0.042612009681761265\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 269, loss: 0.04253508895635605\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 270, loss: 0.04245864413678646\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 271, loss: 0.04238268453627825\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 272, loss: 0.04230719432234764\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 273, loss: 0.04223217815160751\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 274, loss: 0.04215762484818697\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 275, loss: 0.042083531618118286\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 276, loss: 0.04200989939272404\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 277, loss: 0.041936722584068775\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 278, loss: 0.041863990016281605\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 279, loss: 0.041791703552007675\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 280, loss: 0.04171985760331154\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 281, loss: 0.041648452170193195\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 282, loss: 0.0415774742141366\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 283, loss: 0.0415069255977869\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 284, loss: 0.04143681190907955\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 285, loss: 0.04136710334569216\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 286, loss: 0.04129782132804394\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 287, loss: 0.04122895561158657\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 288, loss: 0.04116049688309431\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 289, loss: 0.04109245166182518\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 290, loss: 0.041024806909263134\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 291, loss: 0.040957557037472725\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 292, loss: 0.040890698321163654\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 293, loss: 0.04082424007356167\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 294, loss: 0.04075816739350557\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 295, loss: 0.04069248307496309\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 296, loss: 0.04062718152999878\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 297, loss: 0.040562248788774014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 298, loss: 0.04049770440906286\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 299, loss: 0.04043352697044611\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 300, loss: 0.040369720198214054\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 301, loss: 0.0403062766417861\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 302, loss: 0.04024319909512997\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 303, loss: 0.04018047917634249\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 304, loss: 0.0401181192137301\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 305, loss: 0.04005610244348645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 306, loss: 0.03999444330111146\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 307, loss: 0.039933132007718086\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 308, loss: 0.03987216157838702\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 309, loss: 0.039811539463698864\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 310, loss: 0.039751249831169844\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 311, loss: 0.03969130478799343\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 312, loss: 0.03963168524205685\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 313, loss: 0.039572400506585836\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 314, loss: 0.03951343288645148\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 315, loss: 0.03945480240508914\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 316, loss: 0.039396487176418304\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 317, loss: 0.03933849465101957\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 318, loss: 0.03928081365302205\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 319, loss: 0.03922345256432891\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 320, loss: 0.03916640626266599\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 321, loss: 0.039109665900468826\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 322, loss: 0.03905322588980198\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 323, loss: 0.03899709740653634\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 324, loss: 0.03894126834347844\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 325, loss: 0.038885744754225016\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 326, loss: 0.038830508943647146\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 327, loss: 0.03877556370571256\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 328, loss: 0.038720918353646994\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 329, loss: 0.03866656171157956\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 330, loss: 0.03861248819157481\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 331, loss: 0.03855870896950364\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 332, loss: 0.03850521193817258\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 333, loss: 0.038451984990388155\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 334, loss: 0.03839903883635998\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 335, loss: 0.038346372079104185\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 336, loss: 0.03829398890957236\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 337, loss: 0.03824185533449054\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 338, loss: 0.03819000720977783\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 339, loss: 0.038138422183692455\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 340, loss: 0.03808710863813758\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 341, loss: 0.03803605120629072\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 342, loss: 0.037985258270055056\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 343, loss: 0.03793472796678543\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 344, loss: 0.037884445395320654\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 345, loss: 0.03783442825078964\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 346, loss: 0.03778466256335378\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 347, loss: 0.03773514274507761\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 348, loss: 0.03768586600199342\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 349, loss: 0.03763685096055269\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 350, loss: 0.03758807526901364\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 351, loss: 0.03753954079002142\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 352, loss: 0.03749125683680177\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 353, loss: 0.037443206645548344\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 354, loss: 0.03739539859816432\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 355, loss: 0.03734782570973039\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 356, loss: 0.03730047633871436\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 357, loss: 0.037253379821777344\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 358, loss: 0.03720650356262922\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 359, loss: 0.03715985966846347\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 360, loss: 0.037113435566425323\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 361, loss: 0.03706725174561143\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 362, loss: 0.03702127747237682\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 363, loss: 0.036975536961108446\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 364, loss: 0.03693000925704837\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 365, loss: 0.03688470972701907\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 366, loss: 0.03683961555361748\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 367, loss: 0.0367947556078434\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 368, loss: 0.03675010101869702\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 369, loss: 0.03670565877109766\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 370, loss: 0.036661427933722734\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 371, loss: 0.03661740059033036\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 372, loss: 0.0365735930390656\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 373, loss: 0.036529995035380125\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 374, loss: 0.03648659074679017\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 375, loss: 0.03644339367747307\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 376, loss: 0.03640039870515466\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 377, loss: 0.03635761374607682\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 378, loss: 0.03631502063944936\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 379, loss: 0.03627262683585286\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 380, loss: 0.03623043140396476\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 381, loss: 0.03618843061849475\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 382, loss: 0.03614662494510412\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 383, loss: 0.03610500926151872\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 384, loss: 0.03606358356773853\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 385, loss: 0.036022355780005455\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 386, loss: 0.035981313325464725\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 387, loss: 0.03594045294448733\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 388, loss: 0.035899777431041\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 389, loss: 0.0358592988923192\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 390, loss: 0.03581899730488658\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 391, loss: 0.03577888011932373\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 392, loss: 0.03573894081637263\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 393, loss: 0.035699176136404276\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 394, loss: 0.03565959818661213\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 395, loss: 0.035620194394141436\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 396, loss: 0.03558096941560507\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 397, loss: 0.03554191580042243\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 398, loss: 0.035503041464835405\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 399, loss: 0.03546433476731181\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 400, loss: 0.035425798036158085\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 401, loss: 0.0353874359279871\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 402, loss: 0.03534923540428281\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 403, loss: 0.03531121648848057\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 404, loss: 0.03527334751561284\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 405, loss: 0.03523565083742142\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 406, loss: 0.03519812133163214\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 407, loss: 0.035160756669938564\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 408, loss: 0.03512355266138911\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 409, loss: 0.03508650604635477\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 410, loss: 0.03504961868748069\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 411, loss: 0.03501289803534746\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 412, loss: 0.034976331517100334\n",
      "Test_acc: 1.0\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413, loss: 0.034939922858029604\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 414, loss: 0.03490366414189339\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 415, loss: 0.03486756607890129\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 416, loss: 0.03483161982148886\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 417, loss: 0.03479582816362381\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 418, loss: 0.03476018365472555\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 419, loss: 0.034724696073681116\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 420, loss: 0.03468935890123248\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 421, loss: 0.03465416934341192\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 422, loss: 0.03461912786588073\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 423, loss: 0.03458422888070345\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 424, loss: 0.034549479372799397\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 425, loss: 0.03451488073915243\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 426, loss: 0.03448041435331106\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 427, loss: 0.034446099773049355\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 428, loss: 0.03441192535683513\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 429, loss: 0.03437788784503937\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 430, loss: 0.03434400539845228\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 431, loss: 0.03431024821475148\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 432, loss: 0.03427664004266262\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 433, loss: 0.03424315759912133\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 434, loss: 0.03420982277020812\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 435, loss: 0.03417661972343922\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 436, loss: 0.034143553115427494\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 437, loss: 0.03411061642691493\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 438, loss: 0.03407782083377242\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 439, loss: 0.03404515469446778\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 440, loss: 0.03401262313127518\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 441, loss: 0.03398021450266242\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 442, loss: 0.03394794324412942\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 443, loss: 0.033915800508111715\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 444, loss: 0.03388378769159317\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 445, loss: 0.03385189827531576\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 446, loss: 0.03382013505324721\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 447, loss: 0.03378850221633911\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 448, loss: 0.03375699184834957\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 449, loss: 0.03372562024742365\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 450, loss: 0.03369435667991638\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 451, loss: 0.0336632183752954\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 452, loss: 0.033632205333560705\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 453, loss: 0.033601319417357445\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 454, loss: 0.033570550847798586\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 455, loss: 0.033539891708642244\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 456, loss: 0.03350937506183982\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 457, loss: 0.03347895806655288\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 458, loss: 0.033448667731136084\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 459, loss: 0.03341848077252507\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 460, loss: 0.0333884353749454\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 461, loss: 0.03335848869755864\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 462, loss: 0.03332865796983242\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 463, loss: 0.033298949245363474\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 464, loss: 0.033269344829022884\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 465, loss: 0.03323986101895571\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 466, loss: 0.033210490830242634\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 467, loss: 0.03318122820928693\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 468, loss: 0.0331520838662982\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 469, loss: 0.03312303684651852\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 470, loss: 0.03309411834925413\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 471, loss: 0.033065290190279484\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 472, loss: 0.03303658403456211\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 473, loss: 0.03300798078998923\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 474, loss: 0.03297948790714145\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 475, loss: 0.03295109700411558\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 476, loss: 0.032922811806201935\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 477, loss: 0.03289464022964239\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 478, loss: 0.03286656551063061\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 479, loss: 0.03283860022202134\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 480, loss: 0.03281073598191142\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 481, loss: 0.032782977912575006\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 482, loss: 0.03275531670078635\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 483, loss: 0.03272776119410992\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 484, loss: 0.032700300216674805\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 485, loss: 0.03267295239493251\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 486, loss: 0.032645695842802525\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 487, loss: 0.032618540804833174\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 488, loss: 0.03259149007499218\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 489, loss: 0.03256453899666667\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 490, loss: 0.03253767779096961\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 491, loss: 0.032510919496417046\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 492, loss: 0.032484255731105804\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 493, loss: 0.0324576860293746\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 494, loss: 0.03243121691048145\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 495, loss: 0.032404838129878044\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 496, loss: 0.03237855760380626\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 497, loss: 0.03235237533226609\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 498, loss: 0.03232627362012863\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 499, loss: 0.03230027947574854\n",
      "Test_acc: 1.0\n",
      "--------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线\n",
    "\n",
    "# 导入所需模块\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 导入数据，分别为输入特征和标签\n",
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target\n",
    "\n",
    "# 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）\n",
    "# seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）\n",
    "np.random.seed(116)  # 使用相同的seed，保证输入特征和标签一一对应\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(116)\n",
    "\n",
    "# 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]\n",
    "\n",
    "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
    "x_train = tf.cast(x_train, tf.float32)\n",
    "x_test = tf.cast(x_test, tf.float32)\n",
    "\n",
    "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
    "# 用tf.Variable()标记参数可训练\n",
    "# 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))\n",
    "\n",
    "lr = 0.1  # 学习率为0.1\n",
    "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
    "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
    "epoch = 500  # 循环500轮\n",
    "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和\n",
    "\n",
    "# 训练部分\n",
    "for epoch in range(epoch):  #数据集级别的循环，每个epoch循环一次数据集\n",
    "    for step, (x_train, y_train) in enumerate(train_db):  #batch级别的循环 ，每个step循环一个batch\n",
    "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
    "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
    "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
    "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
    "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
    "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
    "        # 计算loss对各个参数的梯度\n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "\n",
    "        # 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad\n",
    "        w1.assign_sub(lr * grads[0])  # 参数w1自更新\n",
    "        b1.assign_sub(lr * grads[1])  # 参数b自更新\n",
    "\n",
    "    # 每个epoch，打印loss信息\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all/4))\n",
    "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
    "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
    "\n",
    "    # 测试部分\n",
    "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
    "    total_correct, total_number = 0, 0\n",
    "    for x_test, y_test in test_db:\n",
    "        # 使用更新后的参数进行预测\n",
    "        y = tf.matmul(x_test, w1) + b1\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
    "        # 将pred转换为y_test的数据类型\n",
    "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
    "        # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
    "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
    "        # 将每个batch的correct数加起来\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        # 将所有batch中的correct数加起来\n",
    "        total_correct += int(correct)\n",
    "        # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
    "        total_number += x_test.shape[0]\n",
    "    # 总的准确率等于total_correct/total_number\n",
    "    acc = total_correct / total_number\n",
    "    test_acc.append(acc)\n",
    "    print(\"Test_acc:\", acc)\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "# 绘制 loss 曲线\n",
    "plt.title('Loss Function Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Loss')  # y轴变量名称\n",
    "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
    "plt.legend()  # 画出曲线图标\n",
    "plt.show()  # 画出图像\n",
    "\n",
    "# 绘制 Accuracy 曲线\n",
    "plt.title('Acc Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Acc')  # y轴变量名称\n",
    "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、默写，用一层神经网络实现鸢尾花分类。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
